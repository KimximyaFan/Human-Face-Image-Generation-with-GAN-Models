{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz3mb3owYB_e"},"outputs":[],"source":["import kagglehub\n","\n","import os\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy\n","\n","import matplotlib.pyplot as plt\n","\n","import cv2"]},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"ashwingupta3012/human-faces\")\n","\n","print(\"Path to dataset files:\", path)\n","\n","# 다운로드한 데이터셋 경로 확인\n","dataset_path = path + '/Humans'  # kagglehub에서 제공한 경로를 사용\n","print(\"Dataset files:\", os.listdir(dataset_path))"],"metadata":{"id":"ubwzrvl9YKYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download latest version\n","path2 = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n","\n","print(\"Path to dataset files:\", path2)\n","\n","# 다운로드한 데이터셋 경로 확인\n","dataset_path2 = path2 + '/img_align_celeba/img_align_celeba'  # kagglehub에서 제공한 경로를 사용\n","print(\"Dataset files:\", os.listdir(dataset_path2))"],"metadata":{"id":"lAwiJunN5pAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 크기 지정\n","img_size = 64  # 원하는 크기 (예: 64x64)\n","\n","def preprocess_images_with_opencv(dataset_path, img_size):\n","    i = 0\n","\n","    # 데이터셋 폴더 내 이미지 파일 리스트\n","    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.png')]\n","    images = []\n","\n","    if not image_files:\n","        print(\"Warning: No image files found in the dataset path.\")\n","        return np.array([])\n","\n","    for img_file in image_files:\n","        i += 1\n","\n","        if i % 100 == 0:\n","            print(f\"{i}/{str(len(image_files))}\")\n","            if i == 20000:  # 시간상 1000개의 데이터에 대해서만 수행\n","                #pass\n","                break\n","\n","        img_path = os.path.join(dataset_path, img_file)\n","\n","        #컬러\n","        #img = cv2.imread(img_path)  # 이미지를 읽기\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR을 RGB로 변환\n","\n","        #흑백\n","        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # 흑백으로 이미지를 읽기\n","\n","        img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_AREA)  # 크기 조정\n","        img = img.astype(np.float32)  # 데이터 타입 변환\n","        img = (img - 127.5) / 127.5  # [-1, 1]로 정규화\n","\n","        #흑백\n","        img = np.expand_dims(img, axis=-1)  # (height, width) -> (height, width, 1) 형태로 변환\n","        images.append(img)\n","\n","    return np.array(images)"],"metadata":{"id":"cYrgvxgyYM2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_cv1 = preprocess_images_with_opencv(dataset_path, img_size)\n","print(\"Processed images shape:\", images_cv1.shape)\n","\n","plt.imshow(images_cv1[100], cmap='gray')  # 첫 번째 이미지 출력\n","plt.axis('off')  # 축을 제거\n","plt.show()\n","print(images_cv1.shape)"],"metadata":{"id":"j_--04iRYUUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_cv2 = preprocess_images_with_opencv(dataset_path2, img_size)\n","print(\"Processed images shape:\", images_cv2.shape)\n","\n","plt.imshow(images_cv2[100], cmap='gray')  # 첫 번째 이미지 출력\n","plt.axis('off')  # 축을 제거\n","plt.show()\n","print(images_cv2.shape)"],"metadata":{"id":"kpNdK14Q5vBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_cv = np.concatenate((images_cv1, images_cv2), axis=0)"],"metadata":{"id":"u_kkuZCo55We"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(images_cv.shape)\n","plt.imshow(images_cv[25000], cmap='gray')  # 첫 번째 이미지 출력\n","plt.axis('off')  # 축을 제거\n","plt.show()"],"metadata":{"id":"tEr1e05A56ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_generator(latent_dim):\n","    model = tf.keras.Sequential([\n","        # Dense layer to reshape latent vector into a 4x4x512 tensor\n","        layers.Dense(4 * 4 * 512, use_bias=False, input_shape=(latent_dim,)),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","        layers.Reshape((4, 4, 512)),  # Reshaping to 4x4x512\n","\n","        # First transpose convolution to upsample to 8x8x256\n","        layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        # Second transpose convolution to upsample to 16x16x128\n","        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        # Third transpose convolution to upsample to 32x32x64\n","        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        # Final transpose convolution to upsample to 64x64x1\n","        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n","    ])\n","    return model\n","\n","def build_critic():\n","    model = tf.keras.Sequential([\n","        layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same', input_shape=[img_size, img_size, 1]),\n","        layers.LeakyReLU(),\n","\n","        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","\n","        # Flatten and output a single value\n","        layers.Flatten(),\n","        layers.Dropout(0.3),\n","        layers.Dense(1)\n","    ])\n","    return model"],"metadata":{"id":"f6WmzGq1ZNyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gradient_penalty(critic, real_images, fake_images):\n","    batch_size = tf.shape(real_images)[0]\n","    fake_images = fake_images[:batch_size]\n","\n","    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","    interpolated = alpha * real_images + (1 - alpha) * fake_images\n","\n","    with tf.GradientTape() as tape:\n","        tape.watch(interpolated)\n","        interpolated_output = critic(interpolated)\n","\n","    gradients = tape.gradient(interpolated_output, [interpolated])[0]\n","    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n","    penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n","\n","    return penalty"],"metadata":{"id":"JG-lwGN1e1Nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator_loss(fake_output):\n","    return -tf.reduce_mean(fake_output)\n","\n","def critic_loss(real_output, fake_output):\n","    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)"],"metadata":{"id":"HHeJgZ-Te8v-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(real_images, generator, critic, gen_optimizer, crit_optimizer, latent_dim, gp_weight=10):\n","    noise = tf.random.normal([BATCH_SIZE, latent_dim])\n","\n","    # 판별자(critic) 학습\n","    with tf.GradientTape() as crit_tape:\n","        fake_images = generator(noise, training=True)\n","        real_output = critic(real_images, training=True)\n","        fake_output = critic(fake_images, training=True)\n","        print(\"Real images shape:\", real_images.shape)\n","        print(\"Fake images shape:\", fake_images.shape)\n","        gp = gradient_penalty(critic, real_images, fake_images)\n","        crit_loss = critic_loss(real_output, fake_output) + gp_weight * gp\n","\n","    crit_gradients = crit_tape.gradient(crit_loss, critic.trainable_variables)\n","    crit_optimizer.apply_gradients(zip(crit_gradients, critic.trainable_variables))\n","\n","    # 생성자(generator) 학습\n","    with tf.GradientTape() as gen_tape:\n","        fake_images = generator(noise, training=True)\n","        fake_output = critic(fake_images, training=True)\n","        gen_loss = generator_loss(fake_output)\n","\n","    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n","\n","    return crit_loss, gen_loss"],"metadata":{"id":"-DzXc932fAb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(generator, critic, dataset, epochs, latent_dim):\n","    gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n","    crit_optimizer = tf.keras.optimizers.Adam(1e-4)\n","\n","    for epoch in range(epochs):\n","        for real_images in dataset:\n","            crit_loss, gen_loss = train_step(real_images, generator, critic, gen_optimizer, crit_optimizer, latent_dim)\n","        print(f\"Epoch {epoch+1}, Critic Loss: {crit_loss:.4f}, Generator Loss: {gen_loss:.4f}\")"],"metadata":{"id":"IEUgoZocfQu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = images_cv\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 60000\n","dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset"],"metadata":{"id":"IcpRf3_7gEiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LATENT_DIM = 100\n","EPOCHS = 500\n","\n","generator = build_generator(LATENT_DIM)\n","critic = build_critic()\n","\n","train(generator, critic, dataset, EPOCHS, LATENT_DIM)"],"metadata":{"id":"8EE8pedVfV9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_and_save_images(generator, latent_dim, num_examples=16):\n","    noise = tf.random.normal([num_examples, latent_dim])\n","    generated_images = generator(noise, training=False)\n","    generated_images = (generated_images + 1) / 2.0  # [0, 1]로 정규화\n","\n","    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(generated_images[i, :, :, 0], cmap='gray')\n","        ax.axis('off')\n","    plt.show()\n","\n","generate_and_save_images(generator, LATENT_DIM)"],"metadata":{"id":"BHhVF4DRfkaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ERdFLetljZqB"},"execution_count":null,"outputs":[]}]}